{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0cfa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\32721\\anaconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b502237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10dfb4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq_datasets = load_dataset(\"super_glue\",\"boolq\")\n",
    "boolq_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03ca793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设'],\n",
       " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
       "  '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4371c4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'望海楼美国打“台湾牌”是危险的赌博'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2105df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': Value(dtype='string', id=None),\n",
       " 'content': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names\n",
    "datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa78512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5265\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 585\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets[\"train\"]\n",
    "dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc86f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 8484\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = boolq_datasets[\"train\"]\n",
    "dataset.train_test_split(test_size=0.1 , stratify_by_column= \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d786788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].select([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5048483",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterset = datasets[\"train\"].filter(lambda example : \"中国\" in example[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e32bb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['聚焦两会，世界探寻中国成功秘诀',\n",
       " '望海楼中国经济的信心来自哪里',\n",
       " '“中国奇迹”助力世界减贫跑出加速度',\n",
       " '和音瞩目历史交汇点上的中国',\n",
       " '中国风采感染世界',\n",
       " '中国扶贫世界可鉴',\n",
       " '中国梦世界梦',\n",
       " '望海楼中国扩大开放诚意十足',\n",
       " '更加开放的中国更加繁荣的世界',\n",
       " '中国特色粮食安全之路越走越宽广',\n",
       " '美方干预中国内政的图谋绝不会得逞',\n",
       " '“中国制度”的深厚历史底蕴',\n",
       " '当代中国发展进步的根本制度保障',\n",
       " '透过中国开放大门，世界看到了什么？',\n",
       " '弘扬多边主义中国坚守至诚如一',\n",
       " '中国经济有底气行稳致远',\n",
       " '冰雪世界期待中国故事',\n",
       " '祝福中国！祝福世界！',\n",
       " '同心共筑中国梦',\n",
       " '激活中国经济高质量发展的巨大空间',\n",
       " '中国一定能够“过关”',\n",
       " '中国同亚欧以及世界共享发展机遇',\n",
       " '新时代中国与世界共享未来',\n",
       " '更加开放的中国更多机遇的世界',\n",
       " '“把中国人的饭碗牢牢端在自己手中”',\n",
       " '中国的饭碗要端在自己手中',\n",
       " '绑架中国为哪般？',\n",
       " '生命至上是中国抗疫最重要的经验',\n",
       " '开放共赢的中国方案点亮世界',\n",
       " '中国力量中国精神中国效率的集中展现',\n",
       " '新时代中国青年应永怀初心砥砺前行',\n",
       " '办出中国特色世界一流大学',\n",
       " '中国经济逆势而上开新局',\n",
       " '客观认识全球产业链上的中国市场',\n",
       " '阔步走进中国特色社会主义新时代',\n",
       " '中国专利申请跃升世界第一可贵在哪',\n",
       " '海南再次见证中国加大开放的步伐',\n",
       " '努力建设更高水平的平安中国',\n",
       " '减贫伟大奇迹彰显中国制度优势',\n",
       " '为实现全球互惠共享注入“中国力量”',\n",
       " '进取中国博惠世界',\n",
       " '中国抗疫树立人权典范',\n",
       " '抗疫精神是中国人民负重前行的强大动力',\n",
       " '宣言光荣属于英雄的中国人民',\n",
       " '“中国道歉论”荒唐之极',\n",
       " '凝聚建设美丽中国的制度力量',\n",
       " '庆祝新中国成立70周年的精品力作',\n",
       " '从新时代中国特色大国外交成就中汲取智慧和力量',\n",
       " '中国经济在不确定的世界创造确定性',\n",
       " '望海楼老挝国家主席为何两次到中国“下乡”',\n",
       " '中国好人，凝聚奋进力量',\n",
       " '感受小康进程“中国风”',\n",
       " '穿越风雨，撑起中国经济“半壁江山”',\n",
       " '中国大市场谁都唱衰不了',\n",
       " '一堂鲜活生动的新中国历史课',\n",
       " '在劳动奋斗中彰显中国精神',\n",
       " '中国经济的活力澎湃',\n",
       " '为中国特色大国外交提供坚实制度支撑',\n",
       " '让中国市场成为世界的市场',\n",
       " '中国“高颜值”外贸助推扩大开放',\n",
       " '五四运动与中国共产党的诞生',\n",
       " '中美博弈，时与势在中国一边',\n",
       " '为全球制度反腐贡献中国智慧',\n",
       " '70年，中国法律体系逐步健全',\n",
       " '谁是非洲的真朋友？中国行动再次给出答案',\n",
       " '蓬佩奥之流误判了中国更错读了时代',\n",
       " '在利率这件事上，我们看到了中国智慧',\n",
       " '和音感知中国式民主的生动实践',\n",
       " '望海楼面向未来，中国有底气有豪气',\n",
       " '新理念构筑中国发展厚实的底气',\n",
       " '创造人类减贫史上的中国奇迹',\n",
       " '提速降费，升级数字中国',\n",
       " '努力建设中国特色中国风格中国气派的考古学',\n",
       " '共护共度平安温暖中国年',\n",
       " '中国开放的大门越开越大',\n",
       " '中国疫苗“入世”意义非凡',\n",
       " '中国内政不容干涉',\n",
       " '听，超级IP的中国诉说',\n",
       " '中国经济稳中向好何以让世界惊叹',\n",
       " '稳中有进的中国经济有条件实现预期目标',\n",
       " '更高水平开放创造更多“中国红利”',\n",
       " '中国经济发展前景一定会更加光明',\n",
       " '中国自信激荡复兴力',\n",
       " '为实现中国梦贡献青春和力量',\n",
       " '中国空军以开放自信姿态走向世界',\n",
       " '让世界更好认识中国、了解中国',\n",
       " '任何挑战都挡不住中国前进的步伐',\n",
       " '“中国绿”为地球添生机',\n",
       " '这份“来之不易”彰显中国制度优势',\n",
       " '“中国制度”具有自我完善能力',\n",
       " '开放自信的中国军队是维护世界和平的生力军',\n",
       " '2019，中国经济彰显韧劲与潜力',\n",
       " '提振全球气候治理雄心的中国担当',\n",
       " '望海楼中国对绝对贫困的最后一战',\n",
       " '感受美丽中国的发展脉动',\n",
       " '“中国引擎”重启提升全球产业链抗风险能力',\n",
       " '一个中国原则不容挑衅',\n",
       " '中国产品助力全球抗疫',\n",
       " '发挥制度建设的中国智慧',\n",
       " '开放的中国照亮世界期待',\n",
       " '数字经济持续成为驱动中国经济增长关键力量',\n",
       " '|坚定不移走中国道路',\n",
       " '读懂中国共产党历久弥新的密码',\n",
       " '国际发展合作彰显中国责任担当',\n",
       " '光明日报蓬佩奥之流拿病毒起源栽赃陷害中国只会自食苦果',\n",
       " '有中国在，开放不会稀缺',\n",
       " '北京世园会，让世界感知中国',\n",
       " '辛识平用心用情讲好中国故事',\n",
       " '钟声扩大开放，见中国胸怀',\n",
       " '用臆想构建的“美国重建中国论”',\n",
       " '中国抗击美式霸凌的世界意义',\n",
       " '世界需要什么样的联合国？“中国主张”解答“时代之问”',\n",
       " '抓好坚持和完善党的领导坚持和发展中国特色社会主义',\n",
       " '“中国的经验对全人类非常重要”',\n",
       " '中国经济的空间广阔',\n",
       " '让中国成为全球知识价值实现的热土',\n",
       " '望海楼什么是中国当前最重要的事',\n",
       " '新品牌，中国制造乘势而上',\n",
       " '中国的发展中国家地位不容剥夺',\n",
       " '钟声中国坚定反制的立场决不动摇',\n",
       " '中国经济的信心所依',\n",
       " '中国更有自信走向未来',\n",
       " '关键时刻的中国担当',\n",
       " '全球供应链“去中国化”只是噱头',\n",
       " '良好道德是中国精神的基石',\n",
       " '为实现中华民族伟大复兴的中国梦时刻准备着',\n",
       " '五年规划彰显中国制度优势',\n",
       " '如此打压中国企业，很不光彩！',\n",
       " '望海楼主场外交凸显中国感召力',\n",
       " '中国经济长期向好的奥秘',\n",
       " '中国经济的无限潜能',\n",
       " '中国智慧持续助力破解“四大赤字”',\n",
       " '自信的中国更有底气',\n",
       " '我国国家治理依照中国特色社会主义制度展开',\n",
       " '任何对中国的恶意诋毁都是徒劳',\n",
       " '辛识平中国大市场，世界大机遇',\n",
       " '中国市场这么大，欢迎大家都来看看',\n",
       " '指望中国放弃原则立场不啻于痴人说梦',\n",
       " '面对疫情，中国为世界扛起了什么',\n",
       " '中国捍卫核心利益的决心坚定不移',\n",
       " '中国纪检监察报有获得有感情方有获得感',\n",
       " '汶川十年，世界见证中国力量',\n",
       " '用行政手段逼美企撤离中国纯属一厢情愿',\n",
       " '从“两会金句”读懂中国发展',\n",
       " '望海楼全球可持续发展的中国贡献',\n",
       " '“了不起”的女医护见证中国人权发展进步',\n",
       " '中国新时代是世界的机遇',\n",
       " '惠民、利民、为民，建设美丽中国',\n",
       " '自贸试验区扩围展现中国开放新作为',\n",
       " '为建设健康中国增进人民健康福祉作出新贡献',\n",
       " '“安居中国”展现人民情怀',\n",
       " '中国已做好全面应对的准备',\n",
       " '中国将冷静理性反制贸易霸凌',\n",
       " '望海楼什么也撼动不了中国这棵参天大树',\n",
       " '“进博时刻”呈现更加开放的中国',\n",
       " '心系“香山红”扬帆“中国号”',\n",
       " '钟声中国发展绝非外来恩赐',\n",
       " '答好精准扶贫这一世界难题的中国答卷',\n",
       " '中国经济在走上坡路',\n",
       " '新型政党制度彰显“中国优势”',\n",
       " '中国开放之门越开越大',\n",
       " '大变局中的中国与世界',\n",
       " '中国制度为何具有显著优势',\n",
       " '《新中国发展面对面》，化“艰难说服”为“理论解渴”',\n",
       " '人民日报海外版“中国台湾”四个字，踩了谁的痛脚？',\n",
       " '中国经济“磁场”活力不减引力更强',\n",
       " '把握中国经济发展大势',\n",
       " '让中国特色社会主义更有说服力',\n",
       " '中国不断谱写互利共赢新篇章',\n",
       " '无惧疫情“大考”中国经济加速恢复',\n",
       " '人民日报和音复工复产，中国自有底气',\n",
       " '慰问电彰显中国道义（望海楼）',\n",
       " '改革开放是中国的最大底气',\n",
       " '中国之治世界之鉴',\n",
       " '中国精神的时代精华',\n",
       " '中国疫情防控怎么样？国际社会给出公允评价',\n",
       " '中国共产党自我革命的内在逻辑',\n",
       " '从两会热词感受中国脉动',\n",
       " '丁真现象背后，是数字中国新动态',\n",
       " '让中国经济的大海更加壮阔',\n",
       " '开放发展的中国经济扛得住疾风骤雨',\n",
       " '习近平外交思想是新时代中国特色大国外交的根本遵循和行动指南',\n",
       " '推进全球抗疫合作彰显中国担当',\n",
       " '推动中国制造向中高端迈进',\n",
       " '疫情大考中国答卷的世界形象',\n",
       " '为构建亚太命运共同体贡献中国力量',\n",
       " '干预香港事务和中国内政的图谋绝不会得逞',\n",
       " '流动的中国，再次令世人瞩目',\n",
       " '中国智慧和方案对世界贡献巨大',\n",
       " '望海楼“中国天眼”深邃的目光',\n",
       " '望海楼中国经济不惧风险挑战',\n",
       " '筑牢“中国之治”的法治基石',\n",
       " '再说一遍台湾是中国不可分割的一部分！',\n",
       " '中国经济在风浪中稳健前行',\n",
       " '中国有勇闯难关的智慧和力量',\n",
       " '望海楼“进博魅力”的中国答案',\n",
       " '欢迎仪式改革的“中国范儿”',\n",
       " '让中国可持续发展经验造福世界',\n",
       " '望海楼奋进的中国奋斗有你',\n",
       " '中国实力与道义兼具的底气',\n",
       " '战“疫”，我们有中国速度！',\n",
       " '彰显治理现代化的中国优势',\n",
       " '读懂中国经济“超预期”的密码',\n",
       " '中国特色社会主义道路必将越走越宽广',\n",
       " '在实现中国梦的新长征路上奋勇搏击',\n",
       " '树立起中国特色社会主义科学制度体系的里程碑',\n",
       " '美国指责中国知识产权保护不力毫无根据',\n",
       " '让人类社会美好前景在中国大地生动展现',\n",
       " '打造高质量世界经济的中国方案',\n",
       " '只有改革开放才能发展中国',\n",
       " '望海楼武汉必胜！湖北必胜！中国必胜！',\n",
       " '中国抗疫堪称“现场直播”',\n",
       " '民法典标注法治中国新界碑',\n",
       " '钟声干涉中国内政注定不会得逞',\n",
       " '中国经济发展前景一定更加光明',\n",
       " '美丽中国，人人是建设者',\n",
       " '望海楼谱写中国特色大国外交新篇章',\n",
       " '全面呈现新中国70年发展成就',\n",
       " '美国制裁中国军方意欲何为',\n",
       " '向世界讲好中国多党合作故事',\n",
       " '从全国两会读懂中国式民主',\n",
       " '建设更为紧密的中国-东盟命运共同体',\n",
       " '学好新中国史这门必修课',\n",
       " '公立医院凸显中国抗疫优势',\n",
       " '长假检验战疫成效，中国答卷棒棒棒！',\n",
       " '美企增资中国看好巨大市场',\n",
       " '中国经济一定行',\n",
       " '让中国特色社会主义展现更强大的生命力',\n",
       " '感谢美国政客让中国青年更明白奋斗的意义',\n",
       " '中国始终是维护世界粮食安全的积极力量',\n",
       " '夏粮增产，端牢中国饭碗',\n",
       " '“高质量”，中国经济新航向',\n",
       " '这份“中国礼物”为世界带来光芒',\n",
       " '辛识平“尬黑”中国只会让美政客更尴尬',\n",
       " '争创中国特色社会主义实践范例',\n",
       " '外资再次给中国经济投下“信任票”',\n",
       " '奋斗，中国两会的主旋律',\n",
       " '双轮驱动中国经济高质量发展提速',\n",
       " '党的领导是风雨来袭时中国人民最可靠的主心骨',\n",
       " '打造“双循环”的中国经济有力回击“脱钩”谬论',\n",
       " '乘风破浪，中国力量砥柱中流',\n",
       " '开放的“天眼”开放的中国',\n",
       " '人民日报和音完善治理，中国探索提供重要启示',\n",
       " '望海楼浦东开放，三十年激荡中国活力',\n",
       " '望海楼中国经济发展的必由之路',\n",
       " '中国将多点发力进一步保护知识产权',\n",
       " '望海楼中国将进入新发展阶段',\n",
       " '弘扬宪法精神，建设法治中国',\n",
       " '钟声中国国企是平等竞争市场主体',\n",
       " '中国企业有底气战胜挑战持续发展',\n",
       " '“一带一路”源于中国属于世界',\n",
       " '这条文明之路中国永远不会缺席',\n",
       " '保护生态环境建设美丽中国',\n",
       " '望海楼生机勃勃，就是中国的样子',\n",
       " '健康中国的源头活水',\n",
       " '大海依旧在，中国永远在',\n",
       " '白皮书彰显中国应对经贸摩擦的坚定、沉着与理性',\n",
       " '亿万个你我成就中国',\n",
       " '发挥优势，为健康中国贡献力量',\n",
       " '望海楼中国生态之美的点、线、面',\n",
       " '一天100家！中国引资成绩单暖暖的',\n",
       " '不断完善中国特色收入分配理论',\n",
       " '“中国方案”为破解全球治理赤字持续发力',\n",
       " '开放市场，中国跑出加速度',\n",
       " '中国女排！让我们热泪盈眶',\n",
       " '没有任何力量能够阻挡中国人民实现梦想',\n",
       " '以坚定的制度自信开辟中国特色社会主义新境界',\n",
       " '努力建设更高水平的平安中国',\n",
       " '望海楼端牢“中国饭碗”我们底气十足',\n",
       " '中国经济发展前景光明',\n",
       " '中国慕课，促进“互联网+教育公平”',\n",
       " '实地考察后，国际专家一致肯定中国说明了什么？',\n",
       " '疫情不会改变中国经济长期向好基本面',\n",
       " '面对灾难，中国力量最可信赖',\n",
       " '华为绝地反击中国居安思危',\n",
       " '中国外贸再创新高给全球经济注入稳定力量',\n",
       " '关键时刻，中国行动提振全球抗疫信心',\n",
       " '稳中有进的中国经济完全有能力抵御风雨',\n",
       " '积极进取推动中国经济行稳致远',\n",
       " '更均衡、更开放，中国外贸“开门红”不简单',\n",
       " '善于用全媒体讲好中国故事',\n",
       " '开启中国同世界交融发展新画卷',\n",
       " '世界为何感叹中国经济“超预期”?',\n",
       " '保亿万市场主体，汇中国经济“千顷澄碧”',\n",
       " '“感动中国”，带给我们的是感动更是奋进',\n",
       " '破解“四大赤字”的中国方案',\n",
       " '解码“中国之治”的制度优势',\n",
       " '望海楼中国包容性崛起给世界带来机遇',\n",
       " '中国梦一定会实现',\n",
       " '中国经济的巨大韧性熔铸战略定力',\n",
       " '趁早收回干预香港事务、干涉中国内政的政治黑手！',\n",
       " '保护主义阻挡不了中国改革开放步伐',\n",
       " '媒体要向世界讲好中国“脱贫”故事',\n",
       " '夯实法治中国的长远之基',\n",
       " '坚持走中国特色强军之路',\n",
       " '中国“抗疫答卷”彰显制度优势和治理效能',\n",
       " '望海楼中国周边外交又一抹亮色',\n",
       " '“算”出“中国强”',\n",
       " '望海楼为人类健康作出中国贡献',\n",
       " '上合组织的“中国印记”',\n",
       " '夯实中国之治的制度根基',\n",
       " '人民日报钟声中国捍卫国家利益的决心坚定不移',\n",
       " '以习近平新时代中国特色社会主义经济思想引领发展',\n",
       " '为稳定全球供应链贡献中国力量',\n",
       " '钟轩理没有任何力量能够阻挡中国人民实现梦想的步伐',\n",
       " '维护世界和平的中国担当',\n",
       " '永远做中国人民和中华民族的主心骨',\n",
       " '中国中亚合作之花必将开得更鲜艳',\n",
       " '中国北斗，写照自主创新的志气',\n",
       " '望海楼中国经济一定能过关！',\n",
       " '中国开启建造空间站的新时代',\n",
       " '当代中国哲学社会科学大有作为',\n",
       " '中国发展的速度与温度',\n",
       " '用展览讲好中国故事',\n",
       " '辛识平在进博会感悟“看好中国”',\n",
       " '中国军队为维护世界和平注入强劲动能',\n",
       " '中国经济竞争力中见后劲',\n",
       " '深刻把握中国特色反贫困理论',\n",
       " '正确把握中国经济长期向好的大势',\n",
       " '维护国际秩序的“中国贡献”',\n",
       " '互联网时代的中国民法典',\n",
       " '同心战“疫”，向伟大的中国人民致敬！',\n",
       " '走好2019年中国经济“下半程”',\n",
       " '抗疫新阶段中国须内外齐发力',\n",
       " '美国政客“甩锅”中国，既伤理又害己',\n",
       " '蓬佩奥“碰瓷”中国，终将四处碰壁',\n",
       " '讲好中国故事背后的理论',\n",
       " '中国市场潜力将充分激发，为世界各国创造更多需求',\n",
       " '推动中国经济迈向高质量发展新阶段',\n",
       " '“重启”的武汉为中国经济注入信心与动力',\n",
       " '诋毁中国制度的“政治病毒”同样要消除',\n",
       " '望海楼中国发出“圆满收官”动员令',\n",
       " '钟声中国经济发展不是任何鼓噪能否定的',\n",
       " '社区正在成为中国防控疫情的坚强堡垒',\n",
       " '让“一带一路”的中国声音传得更远',\n",
       " '金融业扩大开放尽显“中国底气”',\n",
       " '封堵打压阻滞不了中国科技创新的步伐',\n",
       " '消费时点，释放中国消费潜力',\n",
       " '抗击病毒，中国正扛起人类责任',\n",
       " '外资持续看好中国资本市场',\n",
       " '端牢“中国饭碗”保障粮食安全',\n",
       " '联动合作讲好中国抗疫故事',\n",
       " '松开人才紧箍咒，流动的中国将充满更多活力',\n",
       " '中国空军以开放自信姿态走向世界',\n",
       " '更深拓展中国经济发展新空间',\n",
       " '这场“战疫”让世界深读中国',\n",
       " '坚守扎根中国发展是正确选择',\n",
       " '为新时代中国特色社会主义发展提供宪法保障',\n",
       " '美方用“汇率操纵”施压中国绝不会得逞',\n",
       " '钟声中国与地区国家展现合作大气象',\n",
       " '观察中国经济的一扇新窗',\n",
       " '疫情阻击战打出了中国的真功夫',\n",
       " '坚定砥砺奋进的中国信心',\n",
       " '战疫成效证明中国制度优势',\n",
       " '向世界呈现中国文化之美需要更多李子柒',\n",
       " '更好向世界展示中国理念中国精神中国道路',\n",
       " '“中国制度”具有强大生命力',\n",
       " '“中国奇迹”始终是世界机遇',\n",
       " '中国向建设贸易强国迈出坚实一步',\n",
       " '讲好中国抗疫故事',\n",
       " '中国抗疫体现大国担当',\n",
       " '坚定不移反对任何外部势力干涉中国内政',\n",
       " '共绘美丽中国绿色画卷',\n",
       " '“中国开放的大门只会越开越大”',\n",
       " '中国女篮拼出当下中国的士气',\n",
       " '中国疫苗助力全球战疫',\n",
       " '全球经济治理的中国路径',\n",
       " '中国积极参与全球人权治理',\n",
       " '必须坚持中国共产党领导万众一心风雨无阻向前进',\n",
       " '“放管服”为中国经济加油助力',\n",
       " '改革开放谱写中国人权事业新篇章',\n",
       " '望海楼中国之治大道之行',\n",
       " '中国抗疫的成效岂容抹杀',\n",
       " '“金句里的中国”激荡人心',\n",
       " '中国复工复产稳定全球经济信心',\n",
       " '更加开放的中国与世界同享进步繁荣',\n",
       " '流动的中国活力的中国',\n",
       " '中国外贸稳中提质展现经济韧性潜力',\n",
       " '中国人怎么爱国还需要别人来教吗？',\n",
       " '青年一定能，中国一定行！',\n",
       " '美丽中国，美丽世界',\n",
       " '时代青年成长为中国社会中坚力量',\n",
       " '为建设美丽中国提供制度保障',\n",
       " '从“买买买”感受中国开放的魅力',\n",
       " '种下一片新绿收获美丽中国',\n",
       " '人民日报钟声出于政治私利污蔑中国美方政治偏见误己害人',\n",
       " '钟声没有可以对中国人民颐指气使的教师爷',\n",
       " '中国资本市场迈出双向开放关键一步',\n",
       " '中国经济的底气从何而来？',\n",
       " '锻造公安铁军建设平安中国',\n",
       " '中国特色社会主义政治经济学新的重要理论成果',\n",
       " '深刻把握国家治理现代化的中国逻辑',\n",
       " '望海楼中国发展繁荣是尼泊尔重要机遇',\n",
       " '9亿多网民给中国经济带来活力与动力',\n",
       " '谱写中国特色大国外交时代华章',\n",
       " '抗击疫情彰显中国力量',\n",
       " '探月精神将激励中国为人类福祉作出更多贡献',\n",
       " '“我们对中国经济的前景是乐观的”',\n",
       " '讲好中国故事，展现中国形象',\n",
       " '“人从众”的背后，这些很中国！',\n",
       " '敬礼！中国好老师！',\n",
       " '全民战“疫”，中国对世界的担当',\n",
       " '读懂现代化的“中国魔力”',\n",
       " '污名化中国只能显出美国的孤立与困顿',\n",
       " '“战略性看多中国经济”',\n",
       " '20年，世界为何越来越想听来自这个中国小镇的声音？',\n",
       " '中国发展新引擎推动世界互联互通',\n",
       " '逢山开路遇水架桥，美丽中国一往无前',\n",
       " '中国经济长期向好发展的趋势不会改变',\n",
       " '不断开创法治中国建设新局面',\n",
       " '中国特色减贫道路脱贫攻坚理论结晶',\n",
       " '中国人权答卷广受赞誉',\n",
       " '努力塑造可信可爱可敬的中国形象',\n",
       " '望海楼打造“高质量号”中国经济列车',\n",
       " '创新为中国经济蓄力新动能',\n",
       " '必须坚持中国特色社会主义道路让我们的制度越来越成熟',\n",
       " '钟声中国不会屈服于任何极限施压',\n",
       " '夯实粮食安全之基始终端牢中国饭碗',\n",
       " '用习近平新时代中国特色社会主义思想铸魂育人',\n",
       " '从怀疑到认可，西方一些国家对中国疫苗的态度为何变了？',\n",
       " '中国支援见证“风月同天”',\n",
       " '新时代的中国青年可堪大任！',\n",
       " '完善全球治理中国是“思想家”更是“行动派”',\n",
       " '加征关税无用，中国有能力奉陪到底',\n",
       " '中国抗击疫情伟大斗争的真实叙事',\n",
       " '“中国方案”助力构建网络空间命运共同体',\n",
       " '这张8210亿元春节“账单”见证中国经济活力',\n",
       " '与中国“脱钩”？美国政客的一个“愚蠢”游戏',\n",
       " '望海楼“中国的行动让世界更安全”',\n",
       " '牵手，“中国星”',\n",
       " '中国吸引外资“磁力”强劲',\n",
       " '望海楼中国制度图谱清晰绘就',\n",
       " '中国抗疫的信心、意志与措施更坚定有力',\n",
       " '建设更为紧密的中国－东盟命运共同体',\n",
       " '新证券法助力中国经济转型升级',\n",
       " '中国制度的“最大优势”',\n",
       " '疫情防控是对中国治理能力的一次“大考”',\n",
       " '选择中国就是选择未来',\n",
       " '携手抗疫让中国东盟邻里情更深',\n",
       " '中国在知识产权上不输理',\n",
       " '中国的发展是世界的机遇',\n",
       " '“疫考”下的黄金周愈显中国经济韧性与活力',\n",
       " '为实现中国梦贡献一份电影力量',\n",
       " '稳中有进的中国经济含金量更足',\n",
       " '共享中国机遇造福各国人民',\n",
       " '阔步行进在中国道路上',\n",
       " '任何挑战都挡不住中国前进的步伐',\n",
       " '农丰粮稳展现“中国饭碗”含金量',\n",
       " '中国经济稳中向好、长期向好基本趋势没有改变',\n",
       " '中国减贫故事为何如此动人',\n",
       " '人民日报钟声一个中国原则不容挑战',\n",
       " '美国与中国协商合作才有利彼此更有利世界',\n",
       " '“甩锅”中国成不了美国政客的救命稻草',\n",
       " '中国市场世界机遇',\n",
       " '读懂中国航船的历史方位',\n",
       " '开门问策，“中国式民主”的生动样本',\n",
       " '努力建设新时代中国特色社会主义新疆',\n",
       " '世界共享中国年',\n",
       " '以民法典实施提升“中国之治”',\n",
       " '世界客商云集中国，传递了什么？',\n",
       " '“天问”来了！疫情挡不住中国航天豪情',\n",
       " '望海楼中国将始终践行多边主义',\n",
       " '自信，把握中国未来的关键词',\n",
       " '望海楼利用香港遏制中国是痴人说梦',\n",
       " '钟声履行世贸组织规则，中国言出必行',\n",
       " '以习近平新时代中国特色社会主义经济思想为指导',\n",
       " '为中国人民迸发出来的创造伟力喝彩',\n",
       " '望海楼合作抗疫，中国用行动说话',\n",
       " '中国式民主行得通很管用',\n",
       " '构建“海洋命运共同体”中国海军将继续发力',\n",
       " '为人类和平发展作出中国贡献',\n",
       " '美政客诋毁中国抗疫物资质量是“甩锅”丑剧',\n",
       " '中国将继续为开放型世界经济发展增添动力',\n",
       " '奋进新征程，中国一定“牛”！',\n",
       " '中国抗疫背后的“爱”与“敬”',\n",
       " '老干部是推进中国特色社会主义伟大事业的重要力量',\n",
       " '“中国担责论”在国际法上根本站不住脚',\n",
       " '负面清单再“瘦身”中国开放新突破',\n",
       " '何为命运共同体？中国用行动说话',\n",
       " '致敬浩瀚太空中国追梦人！',\n",
       " '一以贯之坚持和发展中国特色社会主义',\n",
       " '开放的中国为世界经济复苏提供动力',\n",
       " '消费升级为中国经济注入强劲动力',\n",
       " '中国在扩大开放中反击保护主义',\n",
       " '复兴的中国是世界之幸',\n",
       " '钟声中国内政决不容许任何外部势力干涉',\n",
       " '辛识平硬要“撤资中国”坑的是美国企业',\n",
       " '疫情下中国经济长期向好的基本面不会变',\n",
       " '建设美丽中国是我们心向往之的奋斗目标',\n",
       " '消博会“磁力”释放中国开放合作新信号',\n",
       " '中国是世界经济增长的积极贡献者',\n",
       " '望海楼外贸新高背后的中国红利',\n",
       " '“中国之治”展现制度自信',\n",
       " '必须坚持中国特色社会主义道路让我们的制度越来越成熟',\n",
       " '改革开放深刻改变中国深刻影响世界',\n",
       " '任何威胁都吓不倒中国人民',\n",
       " '望海楼新时代的中国青年是好样的',\n",
       " '彰显中国发展的“制度优势”',\n",
       " '马克思主义指引中国成功走上康庄大道',\n",
       " '数字中国，催动发展蝶变',\n",
       " '中国仍有条件巩固经济增长势头',\n",
       " '什么是中国抗疫最深厚的底气',\n",
       " '促进妇女事业中国典范助益世界',\n",
       " '厚植“中国之治”的文化根基',\n",
       " '中国女性为世界妇女事业贡献独特力量',\n",
       " '加速打造“中国芯”',\n",
       " '中国外贸逐步回稳提振全球供应链信心',\n",
       " '望海楼欢迎分享中国市场巨大机遇',\n",
       " '抗疫斗争彰显中国力量',\n",
       " '望海楼G20中国方案值得期待',\n",
       " '为实现中国梦增添强大青春能量',\n",
       " '中国经济的底气所在',\n",
       " '稳中有进，中国经济大趋势',\n",
       " 'GDP破百万亿元中国之“制”凝铸中国之“治”',\n",
       " '读懂中国经济的“含金量”',\n",
       " '深入领会习近平新时代中国特色社会主义思想',\n",
       " '绿色发展点亮中国经济',\n",
       " '世界大变局凸显中国“聚合力”',\n",
       " '中国这个“世界工厂”令美企难以割舍',\n",
       " '和音“中国答卷”提振世界经济复苏信心',\n",
       " '世界瞩目新时代中国力量之源',\n",
       " '中国新冠疫苗筑起免疫防线',\n",
       " '奋力建设更高水平的平安中国法治中国',\n",
       " '坚持和完善中国特色社会主义制度和国家治理体系',\n",
       " '为团结合作抗疫贡献中国力量',\n",
       " '从“一分钟”里读懂中国活力',\n",
       " '望海楼把握中国经济的真实脉动',\n",
       " '所谓“中国垮掉”的妖言为何再次破产？',\n",
       " '人民日报和音“中国答卷”贡献智慧和力量',\n",
       " '干涉中国内政的法案就是废纸一张',\n",
       " '用理性与耐心开启中国资本市场新征程',\n",
       " '和音关键时刻更见中国制度优势',\n",
       " '“中国动力”持续推进建设开放型世界经济',\n",
       " '辛识平美式“人权牌”阻挡不了中国前进',\n",
       " '在中国发展的大势中收获机遇',\n",
       " '中国依然是外商投资热土“外企搬离论”不攻自破',\n",
       " '中国新发展格局的世界意义凸显',\n",
       " '引领人类前途命运的中国方案',\n",
       " '奋力建设良法善治的法治中国',\n",
       " '以优异成绩迎接新中国成立70周年',\n",
       " '普及全民健身运动促进健康中国建设',\n",
       " '让中国开放成果更好惠及世界',\n",
       " '面向未来，中国将永远在这儿',\n",
       " '以科学务实书写抗疫“中国答卷”']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterset[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c724c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(example):\n",
    "    example[\"title\"] = 'Title : ' + example[\"title\"]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d8a222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title : 望海楼美国打“台湾牌”是危险的赌博',\n",
       " 'Title : 大力推进高校治理能力建设',\n",
       " 'Title : 坚持事业为上选贤任能',\n",
       " 'Title : “大朋友”的话儿记心头',\n",
       " 'Title : 用好可持续发展这把“金钥匙”']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_dataset = datasets.map(add_prefix)\n",
    "prefix_dataset[\"train\"][\"title\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b997dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def process_function(example , tokenizer = tokenizer):\n",
    "    model_inputs = tokenizer(example[\"content\"],max_length = 512 , truncation = True)\n",
    "    labels = tokenizer(example[\"title\"], max_length = 32 , truncation = True)\n",
    "    model_inputs[\"label\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3fbcc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1679/1679 [00:02<00:00, 559.88 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dataset = datasets.map(process_function)\n",
    "preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a36182e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dataset = datasets.map(process_function , num_proc= 4)\n",
    "preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f7c4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dataset = datasets.map(process_function , batched= True)\n",
    "preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1391bb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dataset = datasets.map(process_function , batched= True , remove_columns= datasets[\"train\"].column_names)\n",
    "preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93cbe72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5850/5850 [00:00<00:00, 345081.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1679/1679 [00:00<00:00, 231014.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset.save_to_disk(\"processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93774ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data = load_dataset(\"csv\" , data_files= \"ChnSentiCorp_htl_all.csv\" , split= \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e7437cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1bc3ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data = Dataset.from_csv(\"ChnSentiCorp_htl_all.csv\")\n",
    "local_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a4d9c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
       "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
       "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_data = pd.read_csv(\"ChnSentiCorp_htl_all.csv\")\n",
    "local_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18d4273f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data = Dataset.from_pandas(local_data)\n",
    "local_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e78d6a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{\"text\" : \"1\"} , {\"text\" : \"2\"}]\n",
    "Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d817ff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "dataset = load_dataset(\"csv\" , data_files= \"ChnSentiCorp_htl_all.csv\" , split= \"train\")\n",
    "dataset = dataset.filter(lambda example : example[\"review\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96c4c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_function(example):\n",
    "    tokenized_example = tokenizer(example[\"review\"] , max_length= 128 , truncation= True)\n",
    "    tokenized_example[\"labels\"] = example[\"label\"]\n",
    "    return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c1f0749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(processed_function , remove_columns= dataset.column_names , batched= True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37ddd84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[ 101, 4801,  816, 6392, 3177, 6820, 1377,  809, 8024, 3302, 1218, 2523,\n",
       "           100, 1962,  100, 8024,  671,  702, 3241,  677, 4994, 4197, 2802,  749,\n",
       "          1962, 1126,  702, 4510, 6413,  677, 3341, 7309, 7444,  679, 7444, 6206,\n",
       "          2207, 1995, 3302, 1218,  511,  511,  511,  511,  511,  511,  102,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 5318, 2190, 5543, 6397,  677, 3297, 2345, 1963, 2157,  749, 8013,\n",
       "          8013,  122, 8024, 5018,  671, 7313, 2791, 8038, 1057,  857, 1400, 1355,\n",
       "          4385, 5381, 5317,  679, 5543, 4500, 8024, 2823,  782, 5335,  934, 1400,\n",
       "          6158, 1440, 4761, 5296, 6662, 1776,  749, 8024, 6375, 2769, 5632, 2346,\n",
       "          7028, 3173, 3146, 4415, 6121, 3330,  678, 3517, 2940, 2791,  511,  123,\n",
       "          8024, 5018,  753, 7313, 2791, 8038,  671, 2458, 7305, 2218, 3221,  671,\n",
       "          5500, 2460, 1456, 8024, 3209, 3227, 2523,  719, 3680,  782, 3800,  749,\n",
       "           671, 5500, 2487, 4164, 4638, 7450, 1456, 8013, 8013, 2769, 1762, 6624,\n",
       "          2443,  677, 2823, 3302, 1218, 1447, 3418, 3315, 3766,  782, 4415, 2769,\n",
       "          8024, 1372, 5543, 1086, 3613, 2940, 2791, 8013, 2769, 1762, 1184, 1378,\n",
       "          2990, 1139, 2692, 6224, 3418, 3315, 2218,  102],\n",
       "         [ 101, 4761, 6887, 5381, 5296, 2970, 1366, 1762, 1525, 1036, 1408, 8043,\n",
       "          3683, 7770, 2157, 2411, 4638, 1765, 6887, 1366, 6820, 7391, 5929,  511,\n",
       "          1762, 2414, 1928, 3385, 1400, 7481, 8013, 2682,  679, 6887, 1416, 8043,\n",
       "          4692,  872, 2582,  720, 4500,  511,  122, 8038, 5632, 2372,  125, 5101,\n",
       "           809,  677, 5381, 5296, 8039,  123, 8038, 5632, 2372, 2207, 3352, 1135,\n",
       "          8038,  124, 8038, 2791, 7313, 4638, 1796, 1769, 3446, 1377, 5436, 6814,\n",
       "          3341, 1777,  119,  119,  119,  119,  119,  119,  102,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 6821, 2347, 5307, 3221, 2769, 5018,  753, 3613, 1057,  857, 6421,\n",
       "          6983, 2421,  749,  511, 6983, 2421, 3221, 1762, 2356,  704, 2552, 8024,\n",
       "          3766, 2682, 1168, 6821, 3613, 1057,  857, 6983, 2421, 3300,  749, 3788,\n",
       "          6756, 3302, 1218, 8024, 6375,  782, 2697, 6230,  977, 6756, 2523, 3175,\n",
       "           912,  511, 1215, 4415, 1057,  857, 4633, 6381, 3198, 8024, 1184, 1378,\n",
       "          2207, 1995,  738, 4851, 6505, 4178, 2658,  511, 2769,  857, 4638, 2145,\n",
       "          2791, 3221, 3403, 1114,  100, 8024, 2791, 7313,  679, 7231, 8024, 6842,\n",
       "          2791, 5310, 2362, 3198, 6121, 3330, 1447,  712, 1220, 2376, 2769, 2990,\n",
       "          4708, 6121, 3330, 8024, 1184, 1378, 3302, 1218, 1447, 3082,  868, 2571,\n",
       "          2949,  511, 2600, 4638, 3341, 6432, 8024, 6821, 2157, 6983, 2421, 2523,\n",
       "           679, 7231, 8024, 1420, 3302, 1218, 1447,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([0, 0, 1, 1])})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer= tokenizer)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(tokenized_dataset , shuffle= True , batch_size = 4 , collate_fn= collator)\n",
    "\n",
    "next(enumerate(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2da73e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer , AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc66d241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\" , data_files= \"ChnSentiCorp_htl_all.csv\" , split= \"train\")\n",
    "dataset = dataset.filter(lambda example : example[\"review\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dcc8dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size= 0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3978023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6988/6988 [00:01<00:00, 4813.22 examples/s]\n",
      "Map: 100%|██████████| 777/777 [00:00<00:00, 2480.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "tokenized_dataset = datasets.map(processed_function , remove_columns= datasets[\"train\"].column_names , batched= True)\n",
    "\n",
    "trainset = tokenized_dataset[\"train\"]\n",
    "validset = tokenized_dataset[\"test\"]\n",
    "\n",
    "trainloader = DataLoader(trainset , batch_size=32 , shuffle= True , collate_fn= DataCollatorWithPadding(tokenizer=tokenizer) )\n",
    "validloader = DataLoader(validset , batch_size=64 , shuffle= False , collate_fn= DataCollatorWithPadding(tokenizer=tokenizer) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b65a2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  749,  753,  ...,    0,    0,    0],\n",
       "        [ 101,  129, 3299,  ...,    0,    0,    0],\n",
       "        [ 101, 3302, 1218,  ..., 2203, 3143,  102],\n",
       "        ...,\n",
       "        [ 101, 4384, 1862,  ...,    0,    0,    0],\n",
       "        [ 101, 6820, 3683,  ...,    0,    0,    0],\n",
       "        [ 101, 4895, 1765,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(validloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bdf1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch : 0 , global_step: 0, loss: 0.5766072273254395\n",
      " epoch : 0 , global_step: 100, loss: 0.3345502018928528\n",
      " epoch : 0 , global_step: 200, loss: 0.2951265275478363\n",
      "epoch:0 , {'accuracy': 0.8725868725868726, 'f1': 0.9105691056910569, 'precision': 0.8936170212765957, 'recall': 0.9281767955801105}\n",
      " epoch : 1 , global_step: 300, loss: 0.37327492237091064\n",
      " epoch : 1 , global_step: 400, loss: 0.20363463461399078\n",
      "epoch:1 , {'accuracy': 0.8841698841698842, 'f1': 0.9147727272727273, 'precision': 0.9415204678362573, 'recall': 0.8895027624309392}\n",
      " epoch : 2 , global_step: 500, loss: 0.21852561831474304\n",
      " epoch : 2 , global_step: 600, loss: 0.11407290399074554\n",
      "epoch:2 , {'accuracy': 0.8712998712998713, 'f1': 0.9079189686924494, 'precision': 0.9079189686924494, 'recall': 0.9079189686924494}\n",
      "输入 : 这家店不错！ \n",
      " 输出 : 好评\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "\n",
    "optimizer = Adam(model.parameters(),lr= 2e-5)\n",
    "\n",
    "\n",
    "import evaluate\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\" , \"f1\" , \"precision\" , \"recall\"])\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            output = model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim = -1)\n",
    "            clf_metrics.add_batch(predictions= pred.long() , references= batch[\"labels\"].long())\n",
    "    return clf_metrics.compute()\n",
    "\n",
    "def train(epoch = 3 , log_step = 100):\n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0 :\n",
    "                print(f\" epoch : {ep} , global_step: {global_step}, loss: {output.loss.item()}\")\n",
    "            global_step += 1\n",
    "        clf = evaluate()\n",
    "        print(f\"epoch:{ep} , {clf}\")\n",
    "        \n",
    "\n",
    "\n",
    "train()\n",
    "        \n",
    "        \n",
    "sen = \"这家店不错！\"\n",
    "model.eval()\n",
    "id2label = {0: \"差评\" , 1: \"好评\"}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sen , return_tensors= \"pt\")\n",
    "    logits = model(**inputs).logits\n",
    "    pred = torch.argmax(logits , dim = -1)\n",
    "    print(f\"输入 : {sen} \\n 输出 : {id2label.get(pred.item())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
